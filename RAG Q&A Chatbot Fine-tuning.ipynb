{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4525,"sourceType":"datasetVersion","datasetId":2735}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installs\nRestart after installing","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom IPython.display import clear_output\n\n! pip install -qq langchain-huggingface\n! pip install -qq langchain-community\n#! pip install -qq langchain\n! pip install -qq rouge_score\n! pip install -qq bitsandbytes\n#! pip install -qq accelerate\n! pip install faiss-gpu-cu12\n#! pip install -qq peft\n#! pip install -qq torch\n! pip install -qq evaluate\n! pip install -qq trl\n\nclear_output()","metadata":{"_kg_hide-output":false,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom IPython.display import clear_output\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport glob\nimport textwrap\nimport time\nimport pandas as pd\nfrom datasets import Dataset\nimport evaluate\nfrom tqdm import tqdm\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nimport langchain\n\n### loaders\nfrom langchain.document_loaders import DirectoryLoader, TextLoader\n\n### splits\nfrom langchain.text_splitter import SpacyTextSplitter\n\n### prompts\nfrom langchain import PromptTemplate, LLMChain\n\n### vector stores\nfrom langchain.vectorstores import FAISS\n\n### models\nfrom langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n\n### retrievers\nfrom langchain.chains import RetrievalQA\n\nimport torch\nimport transformers\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM,\n    BitsAndBytesConfig, pipeline, GenerationConfig, TrainingArguments,\n    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n)\nfrom trl import SFTTrainer\n\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2025-06-09T17:55:27.543052Z","iopub.execute_input":"2025-06-09T17:55:27.543730Z","iopub.status.idle":"2025-06-09T17:55:56.256331Z","shell.execute_reply.started":"2025-06-09T17:55:27.543702Z","shell.execute_reply":"2025-06-09T17:55:56.255656Z"},"trusted":true},"outputs":[{"name":"stdout","text":"CPU times: user 18.2 s, sys: 3.32 s, total: 21.5 s\nWall time: 28.7 s\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# CFG","metadata":{}},{"cell_type":"code","source":"class CFG:\n    # LLMs\n    model_name = 'NousResearch/Llama-2-7b-chat-hf' # NousResearch/Llama-2-7b-chat-hf, google/flan-t5-base\n    task_type = TaskType.CAUSAL_LM # TaskType.SEQ_2_SEQ_LM for flan-t5, TaskType.CAUSAL_LM for llama2\n    fine_tune_with_LoRA = True\n    lora_target_modules = ['up_proj', 'q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'down_proj']\n    # Flan-T5 [\"q\", \"v\"], Llama ['up_proj', 'q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'down_proj']\n    \n    temperature = 0\n    top_p = 0.95\n    repetition_penalty = 1.15\n\n    # splitting\n    split_chunk_size = 300 # set 300 for Llama; 200 for flan-t5\n    split_overlap = 0\n\n    # similar passages\n    k = 4\n    \n    # Vector Database Embedding\n    embedding_model = 'sentence-transformers/all-mpnet-base-v2'\n    \n    # paths\n    DOCs_path = '/kaggle/input/questionanswer-dataset/text_data/text_data'\n    Output_folder = './rag-vectordb'","metadata":{"execution":{"iopub.status.busy":"2025-06-09T17:55:56.257388Z","iopub.execute_input":"2025-06-09T17:55:56.257990Z","iopub.status.idle":"2025-06-09T17:55:56.262837Z","shell.execute_reply.started":"2025-06-09T17:55:56.257968Z","shell.execute_reply":"2025-06-09T17:55:56.262031Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Preprocess Data","metadata":{}},{"cell_type":"code","source":"df_S08 = pd.read_csv('/kaggle/input/questionanswer-dataset/S08_question_answer_pairs.txt', sep='\\t')\ndf_S08.head()","metadata":{"execution":{"iopub.status.busy":"2025-06-09T17:55:56.263701Z","iopub.execute_input":"2025-06-09T17:55:56.264025Z","iopub.status.idle":"2025-06-09T17:55:56.327814Z","shell.execute_reply.started":"2025-06-09T17:55:56.263996Z","shell.execute_reply":"2025-06-09T17:55:56.327074Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"      ArticleTitle                                           Question Answer  \\\n0  Abraham_Lincoln  Was Abraham Lincoln the sixteenth President of...    yes   \n1  Abraham_Lincoln  Was Abraham Lincoln the sixteenth President of...   Yes.   \n2  Abraham_Lincoln  Did Lincoln sign the National Banking Act of 1...    yes   \n3  Abraham_Lincoln  Did Lincoln sign the National Banking Act of 1...   Yes.   \n4  Abraham_Lincoln                   Did his mother die of pneumonia?     no   \n\n  DifficultyFromQuestioner DifficultyFromAnswerer  ArticleFile  \n0                     easy                   easy  S08_set3_a4  \n1                     easy                   easy  S08_set3_a4  \n2                     easy                 medium  S08_set3_a4  \n3                     easy                   easy  S08_set3_a4  \n4                     easy                 medium  S08_set3_a4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ArticleTitle</th>\n      <th>Question</th>\n      <th>Answer</th>\n      <th>DifficultyFromQuestioner</th>\n      <th>DifficultyFromAnswerer</th>\n      <th>ArticleFile</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Abraham_Lincoln</td>\n      <td>Was Abraham Lincoln the sixteenth President of...</td>\n      <td>yes</td>\n      <td>easy</td>\n      <td>easy</td>\n      <td>S08_set3_a4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Abraham_Lincoln</td>\n      <td>Was Abraham Lincoln the sixteenth President of...</td>\n      <td>Yes.</td>\n      <td>easy</td>\n      <td>easy</td>\n      <td>S08_set3_a4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Abraham_Lincoln</td>\n      <td>Did Lincoln sign the National Banking Act of 1...</td>\n      <td>yes</td>\n      <td>easy</td>\n      <td>medium</td>\n      <td>S08_set3_a4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Abraham_Lincoln</td>\n      <td>Did Lincoln sign the National Banking Act of 1...</td>\n      <td>Yes.</td>\n      <td>easy</td>\n      <td>easy</td>\n      <td>S08_set3_a4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Abraham_Lincoln</td>\n      <td>Did his mother die of pneumonia?</td>\n      <td>no</td>\n      <td>easy</td>\n      <td>medium</td>\n      <td>S08_set3_a4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"print(f\"Before removing NULL values: {df_S08.shape}\")\n\ndf_S08 = df_S08.dropna()\n\nprint(f\"After removing NULL values: {df_S08.shape}\")","metadata":{"execution":{"iopub.status.busy":"2025-06-09T17:55:56.329251Z","iopub.execute_input":"2025-06-09T17:55:56.329463Z","iopub.status.idle":"2025-06-09T17:55:56.337838Z","shell.execute_reply.started":"2025-06-09T17:55:56.329446Z","shell.execute_reply":"2025-06-09T17:55:56.337307Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Before removing NULL values: (1715, 6)\nAfter removing NULL values: (1148, 6)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"df_S08 = df_S08.drop_duplicates(subset=['Question'])\n\nprint(f\"After removing duplicates: {df_S08.shape}\")","metadata":{"execution":{"iopub.status.busy":"2025-06-09T17:55:56.338564Z","iopub.execute_input":"2025-06-09T17:55:56.338830Z","iopub.status.idle":"2025-06-09T17:55:56.354293Z","shell.execute_reply.started":"2025-06-09T17:55:56.338802Z","shell.execute_reply":"2025-06-09T17:55:56.353685Z"},"trusted":true},"outputs":[{"name":"stdout","text":"After removing duplicates: (602, 6)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df_S08['Question'] = df_S08.apply(\n    lambda x: x['Question'] if all(word in x['Question'] for word in x['ArticleTitle'].replace('_', ' ').split()) \n    else x['ArticleTitle'].replace('_', ' ') + \". \" + x['Question'], \n    axis=1\n)","metadata":{"execution":{"iopub.status.busy":"2025-06-09T17:55:56.355004Z","iopub.execute_input":"2025-06-09T17:55:56.355208Z","iopub.status.idle":"2025-06-09T17:55:56.375312Z","shell.execute_reply.started":"2025-06-09T17:55:56.355193Z","shell.execute_reply":"2025-06-09T17:55:56.374770Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Loader","metadata":{}},{"cell_type":"code","source":"loader = DirectoryLoader(\n    CFG.DOCs_path,\n    glob=\"S08*.txt.clean\",\n    loader_cls=TextLoader,\n    show_progress=True,\n    use_multithreading=True,\n    loader_kwargs={\"encoding\": \"ISO-8859-1\"}\n)\n\ndocuments = loader.load()","metadata":{"execution":{"iopub.status.busy":"2025-06-09T17:55:56.376000Z","iopub.execute_input":"2025-06-09T17:55:56.376208Z","iopub.status.idle":"2025-06-09T17:55:56.484008Z","shell.execute_reply.started":"2025-06-09T17:55:56.376193Z","shell.execute_reply":"2025-06-09T17:55:56.483194Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 1011.42it/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(f'We have {len(documents)} pages in total')","metadata":{"execution":{"iopub.status.busy":"2025-06-09T17:55:56.484816Z","iopub.execute_input":"2025-06-09T17:55:56.485053Z","iopub.status.idle":"2025-06-09T17:55:56.489100Z","shell.execute_reply.started":"2025-06-09T17:55:56.485036Z","shell.execute_reply":"2025-06-09T17:55:56.488438Z"},"trusted":true},"outputs":[{"name":"stdout","text":"We have 40 pages in total\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(documents[0].page_content[:600])","metadata":{"execution":{"iopub.status.busy":"2025-06-09T17:55:56.489668Z","iopub.execute_input":"2025-06-09T17:55:56.489832Z","iopub.status.idle":"2025-06-09T17:55:56.503662Z","shell.execute_reply.started":"2025-06-09T17:55:56.489820Z","shell.execute_reply":"2025-06-09T17:55:56.503093Z"},"trusted":true},"outputs":[{"name":"stdout","text":"polar bear\n\n\n\nThe polar bear (Ursus maritimus) is a bear native to the Arctic. Polar bears and Kodiak bears are the world's largest land carnivores, with most adult males weighing 300-600 kg (660-1320 lb); adult females are about half the size of males. Its fur is hollow and translucent, but usually appears as white or cream colored, thus providing the animal with effective camouflage.  Its skin is actually black in color. Its thick blubber and fur insulate it against the cold. The bear has a short tail and small ears that help reduce heat loss, as well as a relatively small head and long, tap\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Function to clean text\ndef clean_text(text):\n    # Replace multiple newlines with a single newline\n    text = re.sub(r'\\n+', '\\n', text)\n    # Replace multiple spaces with a single space\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\n# Clean each document\nfor doc in documents:\n    doc.page_content = clean_text(doc.page_content)\n\nprint(documents[0].page_content[:600])","metadata":{"execution":{"iopub.status.busy":"2025-06-09T17:55:56.505148Z","iopub.execute_input":"2025-06-09T17:55:56.505701Z","iopub.status.idle":"2025-06-09T17:55:56.599090Z","shell.execute_reply.started":"2025-06-09T17:55:56.505684Z","shell.execute_reply":"2025-06-09T17:55:56.598332Z"},"trusted":true},"outputs":[{"name":"stdout","text":"polar bear The polar bear (Ursus maritimus) is a bear native to the Arctic. Polar bears and Kodiak bears are the world's largest land carnivores, with most adult males weighing 300-600 kg (660-1320 lb); adult females are about half the size of males. Its fur is hollow and translucent, but usually appears as white or cream colored, thus providing the animal with effective camouflage. Its skin is actually black in color. Its thick blubber and fur insulate it against the cold. The bear has a short tail and small ears that help reduce heat loss, as well as a relatively small head and long, tapered\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Splitter","metadata":{}},{"cell_type":"code","source":"text_splitter = SpacyTextSplitter(\n    chunk_size=CFG.split_chunk_size,\n    chunk_overlap=CFG.split_overlap\n)\n\ntexts = text_splitter.split_documents(documents)\n\nprint(f'We have created {len(texts)} chunks from {len(documents)} pages')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T17:55:56.599844Z","iopub.execute_input":"2025-06-09T17:55:56.600118Z","iopub.status.idle":"2025-06-09T17:56:20.427075Z","shell.execute_reply.started":"2025-06-09T17:55:56.600091Z","shell.execute_reply":"2025-06-09T17:56:20.426208Z"}},"outputs":[{"name":"stdout","text":"We have created 5384 chunks from 40 pages\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Create Embeddings","metadata":{}},{"cell_type":"code","source":"%%time\n\nvectordb = FAISS.from_documents(\n    texts,\n    HuggingFaceEmbeddings(model_name=CFG.embedding_model)\n)\n\n### persist vector database\nvectordb.save_local(f\"{CFG.Output_folder}/faiss_index_rag\")\n#vectordb = FAISS.load_local(f\"{CFG.Output_folder}/faiss_index_rag\", HuggingFaceEmbeddings(model_name=CFG.embedding_model), allow_dangerous_deserialization=True)\n\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T17:56:23.822715Z","iopub.execute_input":"2025-06-09T17:56:23.823720Z","iopub.status.idle":"2025-06-09T17:56:47.752106Z","shell.execute_reply.started":"2025-06-09T17:56:23.823691Z","shell.execute_reply":"2025-06-09T17:56:47.751315Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 19 s, sys: 3.45 s, total: 22.4 s\nWall time: 23.9 s\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Define model","metadata":{}},{"cell_type":"code","source":"retriever = vectordb.as_retriever(search_kwargs={\"k\": CFG.k, \"search_type\": \"similarity\"})\n\n# Initialize an empty list to store contexts\ncontexts = []\n\n# Loop through each question and fetch its context\nfor question in tqdm(df_S08['Question'], desc=\"Fetching contexts\"):\n\n    results = retriever.invoke(question)\n    \n    # Extract page contents from results and join them as a single string\n    context = \" \".join([doc.page_content for doc in results])\n    \n    # Append the context to the list\n    contexts.append(context)\n\n# Add the contexts list as a new column to the dataframe\ndf_S08['Context'] = contexts\n\n# Display the dataframe to verify\ndf_S08[['Question', 'Context']].head()","metadata":{"execution":{"iopub.status.busy":"2025-06-09T17:56:50.575166Z","iopub.execute_input":"2025-06-09T17:56:50.575469Z","iopub.status.idle":"2025-06-09T17:56:58.602516Z","shell.execute_reply.started":"2025-06-09T17:56:50.575445Z","shell.execute_reply":"2025-06-09T17:56:58.601901Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Fetching contexts: 100%|██████████| 602/602 [00:08<00:00, 75.14it/s]\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                            Question  \\\n0  Was Abraham Lincoln the sixteenth President of...   \n2  Abraham Lincoln. Did Lincoln sign the National...   \n4  Abraham Lincoln. Did his mother die of pneumonia?   \n6  Abraham Lincoln. How many long was Lincoln's f...   \n8  Abraham Lincoln. When did Lincoln begin his po...   \n\n                                             Context  \n0  Abraham Lincoln Abraham Lincoln (February 12, ...  \n2  Also included was the creation of the system o...  \n4  Lincoln was only nine when his mother, then th...  \n6  While in New Orleans, he may have witnessed a ...  \n8  Young Abraham Lincoln Lincoln began his politi...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Was Abraham Lincoln the sixteenth President of...</td>\n      <td>Abraham Lincoln Abraham Lincoln (February 12, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Abraham Lincoln. Did Lincoln sign the National...</td>\n      <td>Also included was the creation of the system o...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Abraham Lincoln. Did his mother die of pneumonia?</td>\n      <td>Lincoln was only nine when his mother, then th...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Abraham Lincoln. How many long was Lincoln's f...</td>\n      <td>While in New Orleans, he may have witnessed a ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Abraham Lincoln. When did Lincoln begin his po...</td>\n      <td>Young Abraham Lincoln Lincoln began his politi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"%%time\n\nmodel_repo = CFG.model_name\n\ndevice_map = \"auto\"\n        \ntokenizer = AutoTokenizer.from_pretrained(model_repo)\n\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\nif CFG.task_type == TaskType.CAUSAL_LM:\n\n    compute_dtype = getattr(torch, \"float16\")\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )\n    \n    def base_model_init():\n        model = AutoModelForCausalLM.from_pretrained(\n        model_repo,\n        quantization_config=bnb_config,\n        device_map=device_map\n        )\n        model.config.use_cache = False\n        model.enable_input_require_grads()\n\n        return model\n        \n    base_model = base_model_init()\n\n    #max_len = base_model.config.max_position_embeddings\n    max_len = 2048\n\n    tokenizer.model_max_length = max_len\n\nelif CFG.task_type == TaskType.SEQ_2_SEQ_LM:\n\n    def base_model_init():\n        model = AutoModelForSeq2SeqLM.from_pretrained(\n            model_repo, \n            torch_dtype=torch.bfloat16, \n            device_map = device_map\n        )\n        return model\n    \n    base_model = base_model_init()\n    \n    max_len = base_model.config.n_positions\n\n    tokenizer.model_max_length = max_len\n\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T17:56:58.603646Z","iopub.execute_input":"2025-06-09T17:56:58.603938Z","iopub.status.idle":"2025-06-09T17:58:07.881620Z","shell.execute_reply.started":"2025-06-09T17:56:58.603919Z","shell.execute_reply":"2025-06-09T17:58:07.880831Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 26.9 s, sys: 31 s, total: 57.9 s\nWall time: 1min 9s\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Prompt","metadata":{}},{"cell_type":"code","source":"prompt_template = \"\"\"\nUse only the following pieces of context to answer the question.\n\n{context}\n\nQuestion: {question}\nAnswer: \"\"\"\n\nif CFG.task_type == TaskType.CAUSAL_LM:\n    # Add tags for Llama2\n    prompt_template = f\"[INST]\\n{prompt_template.strip()} [/INST]\"\n\nPROMPT = PromptTemplate(\n    template = prompt_template, \n    input_variables = [\"context\", \"question\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T17:58:07.882760Z","iopub.execute_input":"2025-06-09T17:58:07.883098Z","iopub.status.idle":"2025-06-09T17:58:07.887186Z","shell.execute_reply.started":"2025-06-09T17:58:07.883078Z","shell.execute_reply":"2025-06-09T17:58:07.886614Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Cross-validation","metadata":{}},{"cell_type":"code","source":"def tokenize_function_SEQ2SEQ(row):\n    formatted_prompt = PROMPT.format(question=row['Question'], context=row['Context'])\n    inputs = tokenizer(formatted_prompt, max_length=max_len, truncation=True)\n    labels = tokenizer(row[\"Answer\"], max_length=max_len, truncation=True)\n\n    return {\n        'input_ids': inputs.input_ids,\n        'attention_mask': inputs.attention_mask,\n        'labels': labels.input_ids\n    }\n    \ndef tokenize_function_CAUSAL_LM(row):\n    formatted_prompt = PROMPT.format(question=row['Question'], context=row['Context'])\n    combined_text = f\"<s>{formatted_prompt} {row['Answer']} </s>\"\n    inputs = tokenizer(combined_text, max_length=max_len, truncation=True)\n    \n    return {\n        'input_ids': inputs.input_ids,\n        'attention_mask': inputs.attention_mask,\n        'labels': inputs.input_ids\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T17:58:07.887784Z","iopub.execute_input":"2025-06-09T17:58:07.888014Z","iopub.status.idle":"2025-06-09T17:58:08.834678Z","shell.execute_reply.started":"2025-06-09T17:58:07.887997Z","shell.execute_reply":"2025-06-09T17:58:08.833760Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"hf_dataset = Dataset.from_pandas(df_S08)\n\nif CFG.fine_tune_with_LoRA:\n    \n    if CFG.task_type == TaskType.CAUSAL_LM:\n        hf_dataset = hf_dataset.map(\n            tokenize_function_CAUSAL_LM,\n        )\n    \n    elif CFG.task_type == TaskType.SEQ_2_SEQ_LM:\n        hf_dataset = hf_dataset.map(\n            tokenize_function_SEQ2SEQ,\n        )\n\ntrain_test_split = hf_dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = train_test_split['train']\neval_dataset = train_test_split['test']\n\nprint(f\"Shapes of the datasets:\")\nprint(f\"Training: {len(train_dataset)} samples\")\nprint(f\"Validation: {len(eval_dataset)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T17:58:08.836304Z","iopub.execute_input":"2025-06-09T17:58:08.836590Z","iopub.status.idle":"2025-06-09T17:58:09.564595Z","shell.execute_reply.started":"2025-06-09T17:58:08.836572Z","shell.execute_reply":"2025-06-09T17:58:09.564007Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/602 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dff2f53f50a144b6977ec3199a36187e"}},"metadata":{}},{"name":"stdout","text":"Shapes of the datasets:\nTraining: 481 samples\nValidation: 121 samples\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Perform PEFT with LoRA","metadata":{}},{"cell_type":"code","source":"%%time\n\npeft_model = None\n\nlora_config = LoraConfig(\n    r=8, # Rank\n    lora_alpha=8,\n    target_modules=CFG.lora_target_modules,\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=CFG.task_type\n)\n\nif CFG.fine_tune_with_LoRA:\n    base_model = base_model_init()\n    peft_model = get_peft_model(base_model, lora_config)\n\n    clear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T17:58:09.565361Z","iopub.execute_input":"2025-06-09T17:58:09.565649Z","iopub.status.idle":"2025-06-09T17:58:31.903248Z","shell.execute_reply.started":"2025-06-09T17:58:09.565608Z","shell.execute_reply":"2025-06-09T17:58:31.902675Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 11.5 s, sys: 10.9 s, total: 22.4 s\nWall time: 22.3 s\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"if CFG.task_type == TaskType.CAUSAL_LM:\n    batch_size = 1\n    gradient_accumulation_steps = 4\n    gradient_checkpointing = True\n    fp16_full_eval = True,\n    fp16 = True,\n    max_grad_norm = 0.3\n    optim = \"paged_adamw_32bit\"\n    logging_steps = 60\n    \nelif CFG.task_type == TaskType.SEQ_2_SEQ_LM:\n    batch_size = 8\n    gradient_accumulation_steps = 1\n    gradient_checkpointing = False\n    fp16_full_eval = False\n    fp16 = False\n    max_grad_norm = 1.0\n    optim = \"adamw_torch\"\n    logging_steps = 15\n\ncommon_args = {\n    \"per_device_train_batch_size\": batch_size,\n    \"per_device_eval_batch_size\": batch_size,\n    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n    \"gradient_checkpointing\": gradient_checkpointing,\n    \"fp16_full_eval\": fp16_full_eval,\n    \"fp16\": fp16,\n    \"max_grad_norm\": max_grad_norm,\n    \"num_train_epochs\": 1,\n    \"learning_rate\": 1e-3,\n    \"optim\": optim,\n    \"lr_scheduler_type\": \"cosine\",\n    \"warmup_ratio\": 0.01,\n    \"logging_steps\": logging_steps,\n    \"report_to\": \"none\",\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T17:58:31.903922Z","iopub.execute_input":"2025-06-09T17:58:31.904133Z","iopub.status.idle":"2025-06-09T17:58:31.909446Z","shell.execute_reply.started":"2025-06-09T17:58:31.904116Z","shell.execute_reply":"2025-06-09T17:58:31.908689Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def create_peft_training_args(output_dir, common_args):\n    \"\"\"Helper function to create appropriate training arguments.\"\"\"\n    if CFG.task_type == TaskType.CAUSAL_LM:\n        return TrainingArguments(**common_args, output_dir=output_dir)\n    elif CFG.task_type == TaskType.SEQ_2_SEQ_LM:\n        return Seq2SeqTrainingArguments(**common_args, output_dir=output_dir)\n\ndef create_peft_trainer(args):\n    \"\"\"Helper function to create the appropriate trainer.\"\"\"\n    \n    if CFG.task_type == TaskType.CAUSAL_LM:\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=tokenizer,\n            mlm=False  # Causal LM doesn't use masked LM\n        )\n        return SFTTrainer(\n            model=peft_model,\n            args=args,\n            peft_config=lora_config,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            processing_class=tokenizer,\n            data_collator=data_collator\n        )\n    elif CFG.task_type == TaskType.SEQ_2_SEQ_LM:\n        data_collator = DataCollatorForSeq2Seq(tokenizer)\n        return Seq2SeqTrainer(\n            model=peft_model,\n            args=args,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            data_collator=data_collator,\n            processing_class=tokenizer,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T17:58:31.910237Z","iopub.execute_input":"2025-06-09T17:58:31.910441Z","iopub.status.idle":"2025-06-09T17:58:31.927673Z","shell.execute_reply.started":"2025-06-09T17:58:31.910426Z","shell.execute_reply":"2025-06-09T17:58:31.927106Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"if CFG.fine_tune_with_LoRA:\n    output_dir = f'./peft-qa-training-{str(int(time.time()))}'\n\n    peft_training_args = create_peft_training_args(output_dir, common_args)\n    peft_trainer = create_peft_trainer(peft_training_args)\n    peft_trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T17:58:31.928425Z","iopub.execute_input":"2025-06-09T17:58:31.928672Z","iopub.status.idle":"2025-06-09T18:18:59.255220Z","shell.execute_reply.started":"2025-06-09T17:58:31.928650Z","shell.execute_reply":"2025-06-09T18:18:59.254601Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/481 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59adc590c66048608d9af8efc002c904"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/121 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ff77ab8a4464fe1ab0107ee33cf24ef"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [120/120 20:15, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>60</td>\n      <td>1.572800</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.189100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"# 🤗 Pipeline & Generation","metadata":{}},{"cell_type":"code","source":"generation_config = GenerationConfig(\n    max_new_tokens=64,\n    temperature=CFG.temperature,\n    top_p=CFG.top_p,\n    repetition_penalty=CFG.repetition_penalty,\n)\n\nif CFG.fine_tune_with_LoRA:\n    peft_model.eval()\nelse:\n    base_model.eval()\n\nif CFG.task_type == TaskType.CAUSAL_LM:\n    task = \"text-generation\"\nelif CFG.task_type == TaskType.SEQ_2_SEQ_LM:\n    task = \"text2text-generation\"\n\npipe = pipeline(\n    task = task,\n    model = peft_model if CFG.fine_tune_with_LoRA else base_model,\n    tokenizer = tokenizer,\n    device_map = device_map,\n    generation_config = generation_config\n)\n\nllm = HuggingFacePipeline(pipeline = pipe)\n\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:19:45.210464Z","iopub.execute_input":"2025-06-09T18:19:45.211286Z","iopub.status.idle":"2025-06-09T18:19:45.230006Z","shell.execute_reply.started":"2025-06-09T18:19:45.211236Z","shell.execute_reply":"2025-06-09T18:19:45.229272Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def check_llm_response(dataset, indx):\n    query = dataset[indx]['Question']\n    context = dataset[indx]['Context']\n\n    # Format the prompt using the question\n    formatted_prompt = PROMPT.format(question=query, context=context)\n    \n    # Use the formatted prompt with the LLM\n    llm_response = llm.invoke(formatted_prompt)\n    \n    if CFG.task_type == TaskType.SEQ_2_SEQ_LM:\n        llm_response = formatted_prompt + llm_response\n    elif CFG.task_type == TaskType.CAUSAL_LM:\n        llm_response = llm_response.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").strip()\n        llm_response = re.sub(r\" +\", \" \", llm_response)\n\n    print(llm_response)\n    print(f\"\\nCorrect Answer: {dataset[indx]['Answer']}\")\n\n#check_llm_response(wrong_ans, 0) # use wrong_ans you find later to check better pipeline args\ncheck_llm_response(eval_dataset, 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:19:54.346338Z","iopub.execute_input":"2025-06-09T18:19:54.346609Z","iopub.status.idle":"2025-06-09T18:19:56.121416Z","shell.execute_reply.started":"2025-06-09T18:19:54.346589Z","shell.execute_reply":"2025-06-09T18:19:56.120613Z"}},"outputs":[{"name":"stdout","text":"Use only the following pieces of context to answer the question.\n\nThe population density, , is among the lowest in the world.\n\nThe most densely populated part of the country is the Quebec City-Windsor Corridor along the Great Lakes and Saint Lawrence River in the southeast. A similar proportion live in urban areas concentrated in the Quebec City-Windsor Corridor (notably: the Greater Golden Horseshoe anchored around Toronto, Montreal, Ottawa, and their environs), the BC Lower Mainland (Vancouver and environs), and the Calgary-Edmonton Corridor in Alberta. Since the mid 1990s, Canada's federal government has posted annual budgetary surpluses and has steadily paid down the national debt.\n\nToronto, Ontario skyline with the CN tower.\n\nToronto is Canada's most populous metropolitan area with 5,113,149 people. The Great Lakes feed the St. Lawrence River (in the southeast) where lowlands host much of Canada's population.\n\nQuestion: Where is the most densely populated part of Canada?\nAnswer: Quebec City-Windsor Corridor\n\nCorrect Answer: The most densely populated part of the country is the Quebec City-Windsor Corridor along the Great Lakes and Saint Lawrence River in the southeast.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# Retriever chain","metadata":{}},{"cell_type":"code","source":"retriever = vectordb.as_retriever(search_kwargs = {\"k\": CFG.k, \"search_type\" : \"similarity\"})\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm = llm,\n    chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n    retriever = retriever, \n    chain_type_kwargs = {\"prompt\": PROMPT},\n    return_source_documents = True,\n    verbose = False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:20:04.461129Z","iopub.execute_input":"2025-06-09T18:20:04.461924Z","iopub.status.idle":"2025-06-09T18:20:04.466852Z","shell.execute_reply.started":"2025-06-09T18:20:04.461889Z","shell.execute_reply":"2025-06-09T18:20:04.466135Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# Post-process outputs","metadata":{}},{"cell_type":"code","source":"def wrap_text_preserve_newlines(text, width=700):\n    # Split the input text into lines based on newline characters\n    lines = text.split('\\n')\n\n    # Wrap each line individually\n    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n\n    # Join the wrapped lines back together using newline characters\n    wrapped_text = '\\n'.join(wrapped_lines)\n\n    return wrapped_text\n\n\ndef process_llm_response(llm_response):\n    ans = wrap_text_preserve_newlines(llm_response['result'])\n    \n    sources_used = ' \\n'.join(\n        [\n            source.metadata['source'].split('/')[-1][:-4]\n            + (' - page: ' + str(source.metadata['page']) if 'page' in source.metadata else '')\n            + (f'\\nContent: {source.page_content}' if CFG.task_type != TaskType.CAUSAL_LM else '')\n            for source in llm_response['source_documents']\n        ]\n    )\n    \n    ans = ans + '\\n\\nSources: \\n' + sources_used\n    return ans\n\ndef get_retrieval_data(llm_response):\n    \n    sources_used = ', '.join([\n        source.metadata['source'].split('/')[-1] \n        for source in llm_response['source_documents']\n    ])\n    sources_used = sources_used.replace('.txt.clean', '')\n\n    context_used = '\\n'.join(\n        [\n            source.page_content\n                for source in llm_response['source_documents']\n        ]\n    )\n\n    return sources_used, context_used","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:20:06.330135Z","iopub.execute_input":"2025-06-09T18:20:06.330418Z","iopub.status.idle":"2025-06-09T18:20:06.336958Z","shell.execute_reply.started":"2025-06-09T18:20:06.330395Z","shell.execute_reply":"2025-06-09T18:20:06.336263Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def llm_ans(query):\n    llm_response = qa_chain.invoke(query)\n    ans = process_llm_response(llm_response)\n    \n    if CFG.task_type == TaskType.SEQ_2_SEQ_LM:\n        ans = f\"Question: {query}\\nLLM Answer: \" + ans\n    elif CFG.task_type == TaskType.CAUSAL_LM:\n        ans = ans.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").strip()\n        ans = re.sub(r\" +\", \" \", ans)\n    \n    return ans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:20:08.954226Z","iopub.execute_input":"2025-06-09T18:20:08.954829Z","iopub.status.idle":"2025-06-09T18:20:08.959245Z","shell.execute_reply.started":"2025-06-09T18:20:08.954805Z","shell.execute_reply":"2025-06-09T18:20:08.958542Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# Evaluations\n- Check model on a single sample\n- Calculate average score on validation dataset\n- Check wrong answers","metadata":{}},{"cell_type":"code","source":"# Load the ROUGE metric\nmetric = evaluate.load(\"rouge\")\n\ndef rouge_recall(correct_answer, prediction):\n    answer_words = correct_answer.lower().split()\n    context_words = set(prediction.lower().split())\n\n    # Avoid division by zero\n    if not answer_words:\n        return 0.0\n\n    match_count = sum(1 for word in answer_words if word in context_words)\n\n    return match_count / len(answer_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:20:09.642463Z","iopub.execute_input":"2025-06-09T18:20:09.642757Z","iopub.status.idle":"2025-06-09T18:20:11.850550Z","shell.execute_reply.started":"2025-06-09T18:20:09.642735Z","shell.execute_reply":"2025-06-09T18:20:11.850015Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13daf9ba5a8c43528126bb034d23f919"}},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"## Retrieval Evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate_retrieval(dataset, indx):\n    query = dataset[indx]['Question']\n    correct_answer = dataset[indx]['Answer']\n    article = dataset[indx]['ArticleFile']\n\n    llm_response = qa_chain.invoke(query)\n    pred_sources, pred_context = get_retrieval_data(llm_response)\n\n    print(f\"Predicted Article: {pred_sources}\")\n    print(f\"Correct Article: {article}\")\n\n    print(f\"\\nPredicted Context: {pred_context}\")\n    print(f\"Answer: {correct_answer}\")\n\n    score = rouge_recall(correct_answer, pred_context)\n    print(f\"\\nROUGE Recall Scores: {score:.3f}\")\n\nevaluate_retrieval(eval_dataset, 4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:20:13.962526Z","iopub.execute_input":"2025-06-09T18:20:13.962784Z","iopub.status.idle":"2025-06-09T18:20:15.232842Z","shell.execute_reply.started":"2025-06-09T18:20:13.962766Z","shell.execute_reply":"2025-06-09T18:20:15.232177Z"}},"outputs":[{"name":"stdout","text":"Predicted Article: S08_set3_a1, S08_set3_a1, S08_set3_a1, S08_set3_a1\nCorrect Article: S08_set3_a1\n\nPredicted Context: During the presidential campaign of 1796 Adams was the presidential candidate of the Federalist Party and Thomas Pinckney, the Governor of South Carolina, his running mate.\n\nThe federalists wanted Adams as their presidential candidate to crush Thomas Jefferson's bid.\nAdams' opponents were former Secretary of State Thomas Jefferson of Virginia, who was joined by Senator Aaron Burr of New York on the Democratic-Republican ticket.\n\nAs was customary, Adams stayed in his home town of Quincy rather than actively campaign for the Presidency.\nBecause of Adams's seniority and the need for a northern president, he was elected as the Federalist nominee for president in 1796, over Thomas Jefferson, the leader of the opposition Democratic-Republican Party.\nFerling (1992) ch 19; Ferling (2004)\n\nIn the election of 1800 John Adams and his running mate, Charles Cotesworth Pinckney went against the Republican duo of Jefferson and Burr.\n\nHamilton tried his hardest to sabotage Adams campaign in hopes of boosting Pinckney's chances of winning the presidency.\nAnswer: The Federalist Party\n\nROUGE Recall Scores: 1.000\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"def evaluate_all_retrievals(dataset):\n    total_score = 0.0\n    correct_article_matches = 0\n    count_for_rouge = 0\n\n    for i in tqdm(range(len(dataset)), desc=\"Evaluating\"):\n        query = dataset[i]['Question']\n        correct_answer = dataset[i]['Answer']\n        article = dataset[i]['ArticleFile']\n\n        llm_response = qa_chain.invoke(query)\n        pred_sources, pred_context = get_retrieval_data(llm_response)\n\n        # Count matches\n        pred_list = [\n            src.strip() for src in pred_sources.split(',') if src.strip()\n        ]\n        if article in pred_list:\n            correct_article_matches += 1\n\n        # Only compute ROUGE recall if answer is not \"yes\" or \"no\"\n        if not any(word in correct_answer.lower() for word in (\"yes\", \"no\")):\n            score = rouge_recall(correct_answer, pred_context)\n            total_score += score\n            count_for_rouge += 1\n\n    avg_score = total_score / count_for_rouge if count_for_rouge > 0 else 0\n    match_accuracy = correct_article_matches / len(dataset)\n\n    print(f\"\\nAverage ROUGE Recall Score (excluding yes/no): {avg_score:.3f}\")\n    print(f\"Article Match Accuracy: {correct_article_matches}/{len(dataset)} ({match_accuracy:.2%})\")\n\nevaluate_all_retrievals(eval_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:20:18.537286Z","iopub.execute_input":"2025-06-09T18:20:18.537836Z","iopub.status.idle":"2025-06-09T18:22:49.614260Z","shell.execute_reply.started":"2025-06-09T18:20:18.537809Z","shell.execute_reply":"2025-06-09T18:22:49.613438Z"}},"outputs":[{"name":"stderr","text":"Evaluating:   7%|▋         | 8/121 [00:11<02:56,  1.57s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nEvaluating: 100%|██████████| 121/121 [02:31<00:00,  1.25s/it]","output_type":"stream"},{"name":"stdout","text":"\nAverage ROUGE Recall Score (excluding yes/no): 0.630\nArticle Match Accuracy: 121/121 (100.00%)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## Answer Evaluation","metadata":{}},{"cell_type":"code","source":"def extract_prediction(llm_output):\n    return llm_output.split(\"Answer:\")[1].split(\"Sources:\")[0].strip()\n\ndef evaluate_answer(dataset, indx):\n    # Get the question and correct answer from the DataFrame\n    query = dataset[indx]['Question']\n    correct_answer = dataset[indx]['Answer']\n\n    # Get the predicted answer from the language model\n    pred_ans = llm_ans(query)\n\n    print(pred_ans)\n    print(f\"\\nCorrect Answer: {correct_answer}\")\n\n    # Compute ROUGE scores\n    rouge_score = metric.compute(\n        predictions=[extract_prediction(pred_ans)],\n        references=[correct_answer],\n        use_stemmer=True\n    )\n\n    print(f\"\\nROUGE Scores: {rouge_score}\")\n\nevaluate_answer(eval_dataset, 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:23:08.633559Z","iopub.execute_input":"2025-06-09T18:23:08.634337Z","iopub.status.idle":"2025-06-09T18:23:09.751743Z","shell.execute_reply.started":"2025-06-09T18:23:08.634299Z","shell.execute_reply":"2025-06-09T18:23:09.750902Z"}},"outputs":[{"name":"stdout","text":"Use only the following pieces of context to answer the question.\n\nRetrieved 28 August 2007.\n\nKangaroos have few natural predators.\n\nThe Thylacine, considered by palaeontologists to have once been a major natural predator of the kangaroo, is now extinct.\n\nOther extinct predators included the Marsupial Lion, Megalania and the Wonambi.\n\nEuropean settlers cut down forests to create vast grasslands for sheep and cattle grazing, added stock watering points in arid areas, and have substantially reduced the number of dingoes.\n\nKangaroos are shy and retiring by nature, and in normal circumstances present no threat to humans.\n\nAlong with dingos and other canids, introduced species like foxes and feral cats also pose a threat to kangaroo populations.\n\nKangaroos and wallabies are adept swimmers, and often flee into waterways if presented with the option.\n\nWedge-tailed Eagles and other raptors usually eat kangaroo carrion.\n\nGoannas and other carnivorous reptiles also pose a danger to smaller kangaroo species when other food sources are lacking.\n\nQuestion: Do kangaroos have many natural predators?\nAnswer: No\n\nSources: \nS08_set1_a1.txt.c \nS08_set1_a1.txt.c \nS08_set1_a1.txt.c \nS08_set1_a1.txt.c\n\nCorrect Answer: no\n\nROUGE Scores: {'rouge1': 1.0, 'rouge2': 0.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Define a function to make predictions\ndef predict(batch, llm_ans, metric):\n    queries = batch['Question']\n    \n    # Get the predicted answers from the model for the entire batch\n    pred_ans = [llm_ans(query) for query in queries]\n    \n    # Extract predictions from LLM output\n    extracted_preds = [extract_prediction(ans) for ans in pred_ans]\n    \n    # Initialize lists to store ROUGE scores\n    scores = []\n    \n    # Calculate ROUGE score for each prediction and store recalls\n    for pred, ref in zip(extracted_preds, batch['Answer']):\n        result = metric.compute(predictions=[pred], references=[ref])\n        scores.append(result['rouge1'])\n    \n    # Return predictions, references, and low recall indices\n    return {\n        'prediction': extracted_preds, \n        'reference': batch['Answer'],\n        'scores': scores\n    }\n\n# Apply the function to all rows in the dataset\npredicted_dataset = eval_dataset.map(\n    lambda batch: predict(batch, llm_ans=llm_ans, metric=metric),\n    batched=True,\n    batch_size=16,\n    desc=\"Processing predictions\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:23:12.525195Z","iopub.execute_input":"2025-06-09T18:23:12.525843Z","iopub.status.idle":"2025-06-09T18:26:22.313481Z","shell.execute_reply.started":"2025-06-09T18:23:12.525817Z","shell.execute_reply":"2025-06-09T18:26:22.312872Z"}},"outputs":[{"name":"stderr","text":"Parameter 'function'=<function <lambda> at 0x79e2e8676b60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing predictions:   0%|          | 0/121 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"779659a5b2304cff93c492b776106681"}},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# Compute the ROUGE score for the entire dataset\nrouge_score = metric.compute(\n    predictions=predicted_dataset['prediction'],\n    references=predicted_dataset['reference'],\n    use_aggregator=True,\n    use_stemmer=True\n)\n\nrr_scores = [\n    rouge_recall(ref, pred)\n    for ref, pred in zip(predicted_dataset['reference'], predicted_dataset['prediction'])\n]\naverage_rouge_recall = sum(rr_scores) / len(rr_scores)\n\nprint(f\"ROUGE Recall Score: {average_rouge_recall}\")\nprint(f\"ROUGE F1 Scores: {rouge_score}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:26:22.314523Z","iopub.execute_input":"2025-06-09T18:26:22.314820Z","iopub.status.idle":"2025-06-09T18:26:22.498124Z","shell.execute_reply.started":"2025-06-09T18:26:22.314799Z","shell.execute_reply":"2025-06-09T18:26:22.497332Z"}},"outputs":[{"name":"stdout","text":"ROUGE Recall Score: 0.6525168038537025\nROUGE F1 Scores: {'rouge1': 0.7125967068612542, 'rouge2': 0.19958677685950416, 'rougeL': 0.7091875548635078, 'rougeLsum': 0.7135553558557723}\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Collect the low indices based on rouge1 < 0.5\nlow_rouge_indices = [i for i, rouge1 in enumerate(predicted_dataset['scores']) if rouge1 < 0.5]\nwrong_ans = eval_dataset.select(low_rouge_indices)\n\n# Display the filtered dataframe\npercentage_wrong = round((len(wrong_ans) / len(eval_dataset)) * 100)\nprint(f\"Number of wrong answers: {len(wrong_ans)} ({percentage_wrong}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:26:22.498999Z","iopub.execute_input":"2025-06-09T18:26:22.499221Z","iopub.status.idle":"2025-06-09T18:26:22.513215Z","shell.execute_reply.started":"2025-06-09T18:26:22.499196Z","shell.execute_reply":"2025-06-09T18:26:22.512488Z"}},"outputs":[{"name":"stdout","text":"Number of wrong answers: 37 (31%)\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"evaluate_answer(wrong_ans, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:26:22.514253Z","iopub.execute_input":"2025-06-09T18:26:22.514693Z","iopub.status.idle":"2025-06-09T18:26:24.444429Z","shell.execute_reply.started":"2025-06-09T18:26:22.514674Z","shell.execute_reply":"2025-06-09T18:26:24.443793Z"}},"outputs":[{"name":"stdout","text":"Use only the following pieces of context to answer the question.\n\nAny spots on the flanks and limbs that have not merged into the mass of swirls and stripes are unusually small and discrete, rather than forming rosettes.\n\nThe face and underparts are paler and dappled like those of ordinary spotted leopards.\n\nWhen making a threat, leopards stretch their backs, depress their ribcages between their shoulder blades so they stick out, and lower their heads (similar to domestic cats).\n\nDuring the day they may lie in bush, on rocks, or in a tree with their tails hanging below the treetops and giving them away.\n\nHead and body length is between 90 and 190 cm, the tail reaches 60 to 110cm.\n\nShoulder height is 45 to 80 cm.\n\nMales are considerably larger than females and weigh 37 to 90 kg compared to 28 to 60 kg for females.\n\nRonald M. Nowak: Walker's Mammals of the World.\n\nThe leopard is an agile and graceful predator.\n\nAlthough smaller than the other members of Panthera, the leopard is still able to take large prey given a massive skull that well utilizes powerful jaw muscles.\n\nIts body is comparatively long for a cat and its legs are short.\n\nQuestion: How long is a leopard's tail?\nAnswer: 60-110 cm\n\nSources: \nS08_set1_a2.txt.c \nS08_set1_a2.txt.c \nS08_set1_a2.txt.c \nS08_set1_a2.txt.c\n\nCorrect Answer: 60 to 110cm\n\nROUGE Scores: {'rouge1': 0.3333333333333333, 'rouge2': 0.0, 'rougeL': 0.3333333333333333, 'rougeLsum': 0.3333333333333333}\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"# Conclusions\n\n- Things I found had the most impact on models output quality in my experiments:\n    - Splitting: chunk size, overlap\n    - Search: k\n    - Pipeline parameters (temperature, top_p, penalty)\n    - Embeddings function\n    - Question with or without title","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}