{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4525,"sourceType":"datasetVersion","datasetId":2735}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installs\nRestart after installing","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom IPython.display import clear_output\n\n! pip install -qq langchain-huggingface\n! pip install -qq langchain-community\n! pip install -qq langchain\n! pip install -qq rouge_score\n! pip install -qq bitsandbytes\n! pip install -qq accelerate\n! pip install -qq faiss-gpu\n! pip install -qq peft\n! pip install -qq torch\n! pip install -qq evaluate\n! pip install -qq trl\n\nclear_output()","metadata":{"_kg_hide-output":false,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom IPython.display import clear_output\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport glob\nimport textwrap\nimport time\nimport pandas as pd\nfrom datasets import Dataset\nimport evaluate\nfrom tqdm import tqdm\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nimport langchain\n\n### loaders\nfrom langchain.document_loaders import DirectoryLoader, TextLoader\n\n### splits\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n### prompts\nfrom langchain import PromptTemplate, LLMChain\n\n### vector stores\nfrom langchain.vectorstores import FAISS\n\n### models\nfrom langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n\n### retrievers\nfrom langchain.chains import RetrievalQA\n\nimport torch\nimport transformers\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM,\n    BitsAndBytesConfig, pipeline, GenerationConfig, TrainingArguments,\n    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n)\nfrom trl import SFTTrainer\n\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2024-12-31T08:51:46.089755Z","iopub.execute_input":"2024-12-31T08:51:46.090191Z","iopub.status.idle":"2024-12-31T08:51:54.457855Z","shell.execute_reply.started":"2024-12-31T08:51:46.090166Z","shell.execute_reply":"2024-12-31T08:51:54.457064Z"},"trusted":true},"outputs":[{"name":"stdout","text":"CPU times: user 7.69 s, sys: 1.05 s, total: 8.73 s\nWall time: 8.36 s\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# CFG","metadata":{}},{"cell_type":"code","source":"class CFG:\n    # LLMs\n    model_name = 'NousResearch/Llama-2-7b-chat-hf' # NousResearch/Llama-2-7b-chat-hf, google/flan-t5-base\n    task_type = TaskType.CAUSAL_LM # TaskType.SEQ_2_SEQ_LM for flan-t5, TaskType.CAUSAL_LM for llama2\n    fine_tune_with_LoRA = True\n    lora_target_modules = ['up_proj', 'q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'down_proj']\n    # Flan-T5 [\"q\", \"v\"], Llama ['up_proj', 'q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'down_proj']\n    \n    temperature = 0\n    top_p = 0.95\n    repetition_penalty = 1.15\n\n    # splitting\n    split_chunk_size = 400\n    split_overlap = 0\n\n    # similar passages\n    k = 2\n    \n    # Vector Database Embedding\n    embedding_model = 'sentence-transformers/all-mpnet-base-v2'\n    \n    # paths\n    DOCs_path = '/kaggle/input/questionanswer-dataset/text_data/text_data'\n    Output_folder = './rag-vectordb'","metadata":{"execution":{"iopub.status.busy":"2024-12-31T08:52:00.098287Z","iopub.execute_input":"2024-12-31T08:52:00.098608Z","iopub.status.idle":"2024-12-31T08:52:00.104544Z","shell.execute_reply.started":"2024-12-31T08:52:00.098577Z","shell.execute_reply":"2024-12-31T08:52:00.103629Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Preprocess Data","metadata":{}},{"cell_type":"code","source":"df_S08 = pd.read_csv('/kaggle/input/questionanswer-dataset/S08_question_answer_pairs.txt', sep='\\t')\ndf_S08.head()","metadata":{"execution":{"iopub.status.busy":"2024-12-31T08:52:10.158409Z","iopub.execute_input":"2024-12-31T08:52:10.158702Z","iopub.status.idle":"2024-12-31T08:52:10.185424Z","shell.execute_reply.started":"2024-12-31T08:52:10.158679Z","shell.execute_reply":"2024-12-31T08:52:10.184545Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"      ArticleTitle                                           Question Answer  \\\n0  Abraham_Lincoln  Was Abraham Lincoln the sixteenth President of...    yes   \n1  Abraham_Lincoln  Was Abraham Lincoln the sixteenth President of...   Yes.   \n2  Abraham_Lincoln  Did Lincoln sign the National Banking Act of 1...    yes   \n3  Abraham_Lincoln  Did Lincoln sign the National Banking Act of 1...   Yes.   \n4  Abraham_Lincoln                   Did his mother die of pneumonia?     no   \n\n  DifficultyFromQuestioner DifficultyFromAnswerer  ArticleFile  \n0                     easy                   easy  S08_set3_a4  \n1                     easy                   easy  S08_set3_a4  \n2                     easy                 medium  S08_set3_a4  \n3                     easy                   easy  S08_set3_a4  \n4                     easy                 medium  S08_set3_a4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ArticleTitle</th>\n      <th>Question</th>\n      <th>Answer</th>\n      <th>DifficultyFromQuestioner</th>\n      <th>DifficultyFromAnswerer</th>\n      <th>ArticleFile</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Abraham_Lincoln</td>\n      <td>Was Abraham Lincoln the sixteenth President of...</td>\n      <td>yes</td>\n      <td>easy</td>\n      <td>easy</td>\n      <td>S08_set3_a4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Abraham_Lincoln</td>\n      <td>Was Abraham Lincoln the sixteenth President of...</td>\n      <td>Yes.</td>\n      <td>easy</td>\n      <td>easy</td>\n      <td>S08_set3_a4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Abraham_Lincoln</td>\n      <td>Did Lincoln sign the National Banking Act of 1...</td>\n      <td>yes</td>\n      <td>easy</td>\n      <td>medium</td>\n      <td>S08_set3_a4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Abraham_Lincoln</td>\n      <td>Did Lincoln sign the National Banking Act of 1...</td>\n      <td>Yes.</td>\n      <td>easy</td>\n      <td>easy</td>\n      <td>S08_set3_a4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Abraham_Lincoln</td>\n      <td>Did his mother die of pneumonia?</td>\n      <td>no</td>\n      <td>easy</td>\n      <td>medium</td>\n      <td>S08_set3_a4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"print(f\"Before removing NULL values: {df_S08.shape}\")\n\ndf_S08 = df_S08.dropna()\n\nprint(f\"After removing NULL values: {df_S08.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-12-31T08:52:10.912263Z","iopub.execute_input":"2024-12-31T08:52:10.912534Z","iopub.status.idle":"2024-12-31T08:52:10.919380Z","shell.execute_reply.started":"2024-12-31T08:52:10.912510Z","shell.execute_reply":"2024-12-31T08:52:10.918638Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Before removing NULL values: (1715, 6)\nAfter removing NULL values: (1148, 6)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"df_S08 = df_S08.drop_duplicates(subset=['Question'])\n\nprint(f\"After removing duplicates: {df_S08.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-12-31T08:52:12.980022Z","iopub.execute_input":"2024-12-31T08:52:12.980366Z","iopub.status.idle":"2024-12-31T08:52:12.986760Z","shell.execute_reply.started":"2024-12-31T08:52:12.980338Z","shell.execute_reply":"2024-12-31T08:52:12.985884Z"},"trusted":true},"outputs":[{"name":"stdout","text":"After removing duplicates: (602, 6)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df_S08['Question'] = df_S08.apply(\n    lambda x: x['Question'] if all(word in x['Question'] for word in x['ArticleTitle'].replace('_', ' ').split()) \n    else x['ArticleTitle'].replace('_', ' ') + \". \" + x['Question'], \n    axis=1\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-31T08:52:13.547216Z","iopub.execute_input":"2024-12-31T08:52:13.547619Z","iopub.status.idle":"2024-12-31T08:52:13.568009Z","shell.execute_reply.started":"2024-12-31T08:52:13.547587Z","shell.execute_reply":"2024-12-31T08:52:13.567022Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Loader","metadata":{}},{"cell_type":"code","source":"loader = DirectoryLoader(\n    CFG.DOCs_path,\n    glob=\"S08*.txt.clean\",\n    loader_cls=TextLoader,\n    show_progress=True,\n    use_multithreading=True,\n    loader_kwargs={\"encoding\": \"ISO-8859-1\"}\n)\n\ndocuments = loader.load()","metadata":{"execution":{"iopub.status.busy":"2024-12-31T08:52:45.765058Z","iopub.execute_input":"2024-12-31T08:52:45.765391Z","iopub.status.idle":"2024-12-31T08:52:45.882713Z","shell.execute_reply.started":"2024-12-31T08:52:45.765366Z","shell.execute_reply":"2024-12-31T08:52:45.882047Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 40/40 [00:00<00:00, 2663.22it/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(f'We have {len(documents)} pages in total')","metadata":{"execution":{"iopub.status.busy":"2024-12-31T08:52:48.350461Z","iopub.execute_input":"2024-12-31T08:52:48.350780Z","iopub.status.idle":"2024-12-31T08:52:48.354812Z","shell.execute_reply.started":"2024-12-31T08:52:48.350751Z","shell.execute_reply":"2024-12-31T08:52:48.353983Z"},"trusted":true},"outputs":[{"name":"stdout","text":"We have 40 pages in total\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(documents[0].page_content[:600])","metadata":{"execution":{"iopub.status.busy":"2024-12-31T08:52:48.690353Z","iopub.execute_input":"2024-12-31T08:52:48.690585Z","iopub.status.idle":"2024-12-31T08:52:48.694446Z","shell.execute_reply.started":"2024-12-31T08:52:48.690565Z","shell.execute_reply":"2024-12-31T08:52:48.693550Z"},"trusted":true},"outputs":[{"name":"stdout","text":"otter\n\n\n\nOtters are amphibious (or in one case aquatic) carnivorous mammals.  The otter subfamily Lutrinae forms part of the family Mustelidae, which also includes weasels, polecats, badgers, as well as others. With 13 species in 7 genera, otters have an almost worldwide distribution.\n\nAn otter's den is called a holt.  Male otters are dog-otters, females are bitches and babies are cubs or pups.  The collective noun romp is sometimes used for a group of otters, being descriptive of their often playful nature.\n\n\n\n\nOtters have long, slim bodies and relatively short limbs, with webbed paws. Most h\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Function to clean text\ndef clean_text(text):\n    # Replace multiple newlines with a single newline\n    text = re.sub(r'\\n+', '\\n', text)\n    # Replace multiple spaces with a single space\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()  # Optional: Strip leading/trailing whitespace\n\n# Clean each document\nfor doc in documents:\n    doc.page_content = clean_text(doc.page_content)\n\nprint(documents[0].page_content[:600])","metadata":{"execution":{"iopub.status.busy":"2024-12-31T08:52:51.746756Z","iopub.execute_input":"2024-12-31T08:52:51.747097Z","iopub.status.idle":"2024-12-31T08:52:51.832010Z","shell.execute_reply.started":"2024-12-31T08:52:51.747068Z","shell.execute_reply":"2024-12-31T08:52:51.831327Z"},"trusted":true},"outputs":[{"name":"stdout","text":"otter Otters are amphibious (or in one case aquatic) carnivorous mammals. The otter subfamily Lutrinae forms part of the family Mustelidae, which also includes weasels, polecats, badgers, as well as others. With 13 species in 7 genera, otters have an almost worldwide distribution. An otter's den is called a holt. Male otters are dog-otters, females are bitches and babies are cubs or pups. The collective noun romp is sometimes used for a group of otters, being descriptive of their often playful nature. Otters have long, slim bodies and relatively short limbs, with webbed paws. Most have sharp c\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Splitter","metadata":{}},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = CFG.split_chunk_size,\n    chunk_overlap = CFG.split_overlap\n)\n\ntexts = text_splitter.split_documents(documents)\n\nprint(f'We have created {len(texts)} chunks from {len(documents)} pages')","metadata":{"execution":{"iopub.status.busy":"2024-12-31T08:54:06.737561Z","iopub.execute_input":"2024-12-31T08:54:06.737868Z","iopub.status.idle":"2024-12-31T08:54:07.010496Z","shell.execute_reply.started":"2024-12-31T08:54:06.737848Z","shell.execute_reply":"2024-12-31T08:54:07.009579Z"},"trusted":true},"outputs":[{"name":"stdout","text":"We have created 3162 chunks from 40 pages\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Create Embeddings","metadata":{}},{"cell_type":"code","source":"%%time\n\nvectordb = FAISS.from_documents(\n    texts,\n    HuggingFaceEmbeddings(model_name=CFG.embedding_model)\n)\n\n### persist vector database\nvectordb.save_local(f\"{CFG.Output_folder}/faiss_index_rag\")\n#vectordb = FAISS.load_local(f\"{CFG.Output_folder}/faiss_index_rag\", HuggingFaceEmbeddings(model_name=CFG.embedding_model), allow_dangerous_deserialization=True)\n\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2024-12-31T08:54:31.782247Z","iopub.execute_input":"2024-12-31T08:54:31.782555Z","iopub.status.idle":"2024-12-31T08:54:53.244865Z","shell.execute_reply.started":"2024-12-31T08:54:31.782530Z","shell.execute_reply":"2024-12-31T08:54:53.243970Z"},"trusted":true},"outputs":[{"name":"stdout","text":"CPU times: user 22.8 s, sys: 219 ms, total: 23 s\nWall time: 21.5 s\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"retriever = vectordb.as_retriever(search_kwargs={\"k\": CFG.k, \"search_type\": \"similarity\"})\n\n# Initialize an empty list to store contexts\ncontexts = []\n\n# Loop through each question and fetch its context\nfor question in tqdm(df_S08['Question'], desc=\"Fetching contexts\"):\n\n    results = retriever.invoke(question)\n    \n    # Extract page contents from results and join them as a single string\n    context = \" \".join([doc.page_content for doc in results])\n    \n    # Append the context to the list\n    contexts.append(context)\n\n# Add the contexts list as a new column to the dataframe\ndf_S08['Context'] = contexts\n\n# Display the dataframe to verify\ndf_S08[['Question', 'Context']].head()","metadata":{"execution":{"iopub.status.busy":"2024-12-31T08:56:20.783105Z","iopub.execute_input":"2024-12-31T08:56:20.783483Z","iopub.status.idle":"2024-12-31T08:56:29.971885Z","shell.execute_reply.started":"2024-12-31T08:56:20.783451Z","shell.execute_reply":"2024-12-31T08:56:29.971198Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Fetching contexts: 100%|██████████| 602/602 [00:09<00:00, 65.62it/s]\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                            Question  \\\n0  Was Abraham Lincoln the sixteenth President of...   \n2  Abraham Lincoln. Did Lincoln sign the National...   \n4  Abraham Lincoln. Did his mother die of pneumonia?   \n6  Abraham Lincoln. How many long was Lincoln's f...   \n8  Abraham Lincoln. When did Lincoln begin his po...   \n\n                                             Context  \n0  Abraham Lincoln Abraham Lincoln (February 12, ...  \n2  Transcontinental Railroad, which was completed...  \n4  born. Theodore Roosevelt's mother Mittie died ...  \n6  a frequent visitor to Kentucky, he would have ...  \n8  not like killing animals, even for food. Thoug...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Was Abraham Lincoln the sixteenth President of...</td>\n      <td>Abraham Lincoln Abraham Lincoln (February 12, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Abraham Lincoln. Did Lincoln sign the National...</td>\n      <td>Transcontinental Railroad, which was completed...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Abraham Lincoln. Did his mother die of pneumonia?</td>\n      <td>born. Theodore Roosevelt's mother Mittie died ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Abraham Lincoln. How many long was Lincoln's f...</td>\n      <td>a frequent visitor to Kentucky, he would have ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Abraham Lincoln. When did Lincoln begin his po...</td>\n      <td>not like killing animals, even for food. Thoug...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# Define model","metadata":{}},{"cell_type":"code","source":"%%time\n\nmodel_repo = CFG.model_name\n\ndevice_map = \"auto\"\n        \ntokenizer = AutoTokenizer.from_pretrained(model_repo)\n\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\nif CFG.task_type == TaskType.CAUSAL_LM:\n\n    compute_dtype = getattr(torch, \"float16\")\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )\n    \n    def base_model_init():\n        model = AutoModelForCausalLM.from_pretrained(\n        model_repo,\n        quantization_config=bnb_config,\n        device_map=device_map\n        )\n        model.config.use_cache = False\n        model.enable_input_require_grads()\n\n        return model\n        \n    base_model = base_model_init()\n\n    #max_len = base_model.config.max_position_embeddings\n    max_len = 1024\n\n    tokenizer.model_max_length = max_len\n\nelif CFG.task_type == TaskType.SEQ_2_SEQ_LM:\n\n    def base_model_init():\n        model = AutoModelForSeq2SeqLM.from_pretrained(\n            model_repo, \n            torch_dtype=torch.bfloat16, \n            device_map = device_map\n        )\n        return model\n    \n    base_model = base_model_init()\n    \n    max_len = base_model.config.n_positions\n\n    tokenizer.model_max_length = max_len\n\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2024-12-31T08:56:36.408134Z","iopub.execute_input":"2024-12-31T08:56:36.408435Z","iopub.status.idle":"2024-12-31T08:57:50.768180Z","shell.execute_reply.started":"2024-12-31T08:56:36.408410Z","shell.execute_reply":"2024-12-31T08:57:50.767344Z"},"trusted":true},"outputs":[{"name":"stdout","text":"CPU times: user 18.9 s, sys: 22.8 s, total: 41.7 s\nWall time: 1min 14s\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Prompt","metadata":{}},{"cell_type":"code","source":"prompt_template = \"\"\"\nUse only the following pieces of context to answer the question.\n\n{context}\n\nQuestion: {question}\nAnswer: \"\"\"\n\nif CFG.task_type == TaskType.CAUSAL_LM:\n    # Add tags for Llama2\n    prompt_template = f\"[INST]\\n{prompt_template.strip()} [/INST]\"\n\nPROMPT = PromptTemplate(\n    template = prompt_template, \n    input_variables = [\"context\", \"question\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-31T08:59:24.887668Z","iopub.execute_input":"2024-12-31T08:59:24.888034Z","iopub.status.idle":"2024-12-31T08:59:24.892160Z","shell.execute_reply.started":"2024-12-31T08:59:24.888004Z","shell.execute_reply":"2024-12-31T08:59:24.891237Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Cross-validation","metadata":{}},{"cell_type":"code","source":"def tokenize_function_SEQ2SEQ(row):\n    formatted_prompt = PROMPT.format(question=row['Question'], context=row['Context'])\n    inputs = tokenizer(formatted_prompt, max_length=max_len, truncation=True)\n    labels = tokenizer(row[\"Answer\"], max_length=max_len, truncation=True)\n\n    return {\n        'input_ids': inputs.input_ids,\n        'attention_mask': inputs.attention_mask,\n        'labels': labels.input_ids\n    }\n    \ndef tokenize_function_CAUSAL_LM(row):\n    formatted_prompt = PROMPT.format(question=row['Question'], context=row['Context'])\n    combined_text = f\"<s>{formatted_prompt} {row['Answer']} </s>\"\n    inputs = tokenizer(combined_text, max_length=max_len, truncation=True)\n    \n    return {\n        'input_ids': inputs.input_ids,\n        'attention_mask': inputs.attention_mask,\n        'labels': inputs.input_ids\n    }","metadata":{"execution":{"iopub.status.busy":"2024-12-31T09:01:56.702792Z","iopub.execute_input":"2024-12-31T09:01:56.703113Z","iopub.status.idle":"2024-12-31T09:01:56.708477Z","shell.execute_reply.started":"2024-12-31T09:01:56.703089Z","shell.execute_reply":"2024-12-31T09:01:56.707555Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":"hf_dataset = Dataset.from_pandas(df_S08)\n\nif CFG.fine_tune_with_LoRA:\n    \n    if CFG.task_type == TaskType.CAUSAL_LM:\n        hf_dataset = hf_dataset.map(\n            tokenize_function_CAUSAL_LM,\n        )\n    \n    elif CFG.task_type == TaskType.SEQ_2_SEQ_LM:\n        hf_dataset = hf_dataset.map(\n            tokenize_function_SEQ2SEQ,\n        )\n\ntrain_test_split = hf_dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = train_test_split['train']\neval_dataset = train_test_split['test']\n\nprint(f\"Shapes of the datasets:\")\nprint(f\"Training: {len(train_dataset)} samples\")\nprint(f\"Validation: {len(eval_dataset)} samples\")","metadata":{"execution":{"iopub.status.busy":"2024-12-31T09:01:58.164316Z","iopub.execute_input":"2024-12-31T09:01:58.164579Z","iopub.status.idle":"2024-12-31T09:01:58.755618Z","shell.execute_reply.started":"2024-12-31T09:01:58.164558Z","shell.execute_reply":"2024-12-31T09:01:58.754967Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/602 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0570b1dd8078489384b8c61ac0c8bbf4"}},"metadata":{}},{"name":"stdout","text":"Shapes of the datasets:\nTraining: 481 samples\nValidation: 121 samples\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# Perform PEFT with LoRA","metadata":{}},{"cell_type":"code","source":"%%time\n\npeft_model = None\n\nlora_config = LoraConfig(\n    r=8, # Rank\n    lora_alpha=32,\n    target_modules=CFG.lora_target_modules,\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=CFG.task_type\n)\n\nif CFG.fine_tune_with_LoRA:\n    base_model = base_model_init()\n    peft_model = get_peft_model(base_model, lora_config)\n\n    clear_output()","metadata":{"execution":{"iopub.status.busy":"2024-12-31T09:02:00.014378Z","iopub.execute_input":"2024-12-31T09:02:00.014643Z","iopub.status.idle":"2024-12-31T09:02:06.767626Z","shell.execute_reply.started":"2024-12-31T09:02:00.014620Z","shell.execute_reply":"2024-12-31T09:02:06.766721Z"},"trusted":true},"outputs":[{"name":"stdout","text":"CPU times: user 5.7 s, sys: 1.29 s, total: 6.99 s\nWall time: 6.75 s\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"if CFG.task_type == TaskType.CAUSAL_LM:\n    batch_size = 1\n    gradient_accumulation_steps = 4\n    gradient_checkpointing = True\n    fp16_full_eval = True,\n    fp16 = True,\n    max_grad_norm = 0.3\n    optim = \"paged_adamw_32bit\"\n    logging_steps = 60\n    \nelif CFG.task_type == TaskType.SEQ_2_SEQ_LM:\n    batch_size = 8\n    gradient_accumulation_steps = 1\n    gradient_checkpointing = False\n    fp16_full_eval = False\n    fp16 = False\n    max_grad_norm = 1.0\n    optim = \"adamw_torch\"\n    logging_steps = 15\n\ncommon_args = {\n    \"per_device_train_batch_size\": batch_size,\n    \"per_device_eval_batch_size\": batch_size,\n    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n    \"gradient_checkpointing\": gradient_checkpointing,\n    \"fp16_full_eval\": fp16_full_eval,\n    \"fp16\": fp16,\n    \"max_grad_norm\": max_grad_norm,\n    \"num_train_epochs\": 1,\n    \"learning_rate\": 1e-3,\n    \"optim\": optim,\n    \"lr_scheduler_type\": \"cosine\",\n    \"warmup_ratio\": 0.01,\n    \"evaluation_strategy\": \"steps\",\n    \"logging_steps\": logging_steps,\n    \"report_to\": \"none\",\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:02:08.506219Z","iopub.execute_input":"2024-12-31T09:02:08.506545Z","iopub.status.idle":"2024-12-31T09:02:08.512087Z","shell.execute_reply.started":"2024-12-31T09:02:08.506508Z","shell.execute_reply":"2024-12-31T09:02:08.511186Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def create_peft_training_args(output_dir, common_args):\n    \"\"\"Helper function to create appropriate training arguments.\"\"\"\n    if CFG.task_type == TaskType.CAUSAL_LM:\n        return TrainingArguments(**common_args, output_dir=output_dir)\n    elif CFG.task_type == TaskType.SEQ_2_SEQ_LM:\n        return Seq2SeqTrainingArguments(**common_args, output_dir=output_dir)\n\ndef create_peft_trainer(args):\n    \"\"\"Helper function to create the appropriate trainer.\"\"\"\n    \n    if CFG.task_type == TaskType.CAUSAL_LM:\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=tokenizer,\n            mlm=False  # Causal LM doesn't use masked LM\n        )\n        return SFTTrainer(\n            model=peft_model,\n            args=args,\n            peft_config=lora_config,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            processing_class=tokenizer,\n            data_collator=data_collator\n        )\n    elif CFG.task_type == TaskType.SEQ_2_SEQ_LM:\n        data_collator = DataCollatorForSeq2Seq(tokenizer)\n        return Seq2SeqTrainer(\n            model=peft_model,\n            args=args,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            data_collator=data_collator,\n            processing_class=tokenizer,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:02:09.491362Z","iopub.execute_input":"2024-12-31T09:02:09.491597Z","iopub.status.idle":"2024-12-31T09:02:09.496848Z","shell.execute_reply.started":"2024-12-31T09:02:09.491577Z","shell.execute_reply":"2024-12-31T09:02:09.496064Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"if CFG.fine_tune_with_LoRA:\n    output_dir = f'./peft-qa-training-{str(int(time.time()))}'\n\n    peft_training_args = create_peft_training_args(output_dir, common_args)\n    peft_trainer = create_peft_trainer(peft_training_args)\n    peft_trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:02:10.716854Z","iopub.execute_input":"2024-12-31T09:02:10.717176Z","iopub.status.idle":"2024-12-31T09:14:01.277754Z","shell.execute_reply.started":"2024-12-31T09:02:10.717150Z","shell.execute_reply":"2024-12-31T09:14:01.276993Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [120/120 11:45, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>60</td>\n      <td>1.717400</td>\n      <td>1.423171</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.265500</td>\n      <td>1.251027</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":35},{"cell_type":"markdown","source":"# 🤗 Pipeline & Generation","metadata":{}},{"cell_type":"code","source":"generation_config = GenerationConfig(\n    max_new_tokens=64,\n    temperature=CFG.temperature,\n    top_p=CFG.top_p,\n    repetition_penalty=CFG.repetition_penalty,\n)\n\nif CFG.fine_tune_with_LoRA:\n    peft_model.eval()\nelse:\n    base_model.eval()\n\nif CFG.task_type == TaskType.CAUSAL_LM:\n    task = \"text-generation\"\nelif CFG.task_type == TaskType.SEQ_2_SEQ_LM:\n    task = \"text2text-generation\"\n\npipe = pipeline(\n    task = task,\n    model = peft_model if CFG.fine_tune_with_LoRA else base_model,\n    tokenizer = tokenizer,\n    device_map = device_map,\n    generation_config = generation_config\n)\n\nllm = HuggingFacePipeline(pipeline = pipe)\n\nclear_output()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:15:54.309500Z","iopub.execute_input":"2024-12-31T09:15:54.309841Z","iopub.status.idle":"2024-12-31T09:15:54.329238Z","shell.execute_reply.started":"2024-12-31T09:15:54.309816Z","shell.execute_reply":"2024-12-31T09:15:54.328333Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def check_llm_response(dataset, indx):\n    query = dataset[indx]['Question']\n    context = dataset[indx]['Context']\n\n    # Format the prompt using the question\n    formatted_prompt = PROMPT.format(question=query, context=context)\n    \n    # Use the formatted prompt with the LLM\n    llm_response = llm.invoke(formatted_prompt)\n    \n    if CFG.task_type == TaskType.SEQ_2_SEQ_LM:\n        llm_response = formatted_prompt + llm_response\n    elif CFG.task_type == TaskType.CAUSAL_LM:\n        llm_response = llm_response.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").strip()\n        llm_response = re.sub(r\" +\", \" \", llm_response)\n\n    print(llm_response)\n    print(f\"\\nCorrect Answer: {dataset[indx]['Answer']}\")\n\n#check_llm_response(wrong_ans, 0) # use wrong_ans you find later to check better pipeline args\ncheck_llm_response(eval_dataset, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:18:02.682581Z","iopub.execute_input":"2024-12-31T09:18:02.682941Z","iopub.status.idle":"2024-12-31T09:18:04.172506Z","shell.execute_reply.started":"2024-12-31T09:18:02.682880Z","shell.execute_reply":"2024-12-31T09:18:04.171782Z"}},"outputs":[{"name":"stdout","text":"Use only the following pieces of context to answer the question.\n\ntail reaches 60 to 110cm. Shoulder height is 45 to 80 cm. Males are considerably larger than females and weigh 37 to 90 kg compared to 28 to 60 kg for females. Ronald M. Nowak: Walker's Mammals of the World. Johns Hopkins University Press, 1999 ISBN 0-8018-5789-9 One of many spotted cats, a leopard may be mistaken for a cheetah or a jaguar. The leopard has rosettes rather than cheetah's simple puá¹á¸Ã¡rÄ«ka (\"tiger\", among other things), then borrowed into Greek. The leopard is an agile and graceful predator. Although smaller than the other members of Panthera, the leopard is still able to take large prey given a massive skull that well utilizes powerful jaw muscles. Its body is comparatively long for a cat and its legs are short. Head and body length is between 90 and 190 cm, the\n\nQuestion: How long is a leopard's tail?\nAnswer: 60 to 110 cm\n\nCorrect Answer: 60 to 110cm\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"# Retriever chain","metadata":{}},{"cell_type":"code","source":"retriever = vectordb.as_retriever(search_kwargs = {\"k\": CFG.k, \"search_type\" : \"similarity\"})\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm = llm,\n    chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n    retriever = retriever, \n    chain_type_kwargs = {\"prompt\": PROMPT},\n    return_source_documents = True,\n    verbose = False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:18:13.451117Z","iopub.execute_input":"2024-12-31T09:18:13.451472Z","iopub.status.idle":"2024-12-31T09:18:13.456639Z","shell.execute_reply.started":"2024-12-31T09:18:13.451440Z","shell.execute_reply":"2024-12-31T09:18:13.455693Z"}},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":"# Post-process outputs","metadata":{}},{"cell_type":"code","source":"def wrap_text_preserve_newlines(text, width=700):\n    # Split the input text into lines based on newline characters\n    lines = text.split('\\n')\n\n    # Wrap each line individually\n    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n\n    # Join the wrapped lines back together using newline characters\n    wrapped_text = '\\n'.join(wrapped_lines)\n\n    return wrapped_text\n\n\ndef process_llm_response(llm_response):\n    ans = wrap_text_preserve_newlines(llm_response['result'])\n    \n    sources_used = ' \\n'.join(\n        [\n            source.metadata['source'].split('/')[-1][:-4]\n            + (' - page: ' + str(source.metadata['page']) if 'page' in source.metadata else '')\n            + (f'\\nContent: {source.page_content}' if CFG.fine_tune_with_LoRA else '')\n            for source in llm_response['source_documents']\n        ]\n    )\n    \n    ans = ans + '\\n\\nSources: \\n' + sources_used\n    return ans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:18:18.672799Z","iopub.execute_input":"2024-12-31T09:18:18.673140Z","iopub.status.idle":"2024-12-31T09:18:18.678634Z","shell.execute_reply.started":"2024-12-31T09:18:18.673114Z","shell.execute_reply":"2024-12-31T09:18:18.677627Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"def llm_ans(query):\n    llm_response = qa_chain.invoke(query)\n    ans = process_llm_response(llm_response)\n    \n    if CFG.task_type == TaskType.SEQ_2_SEQ_LM:\n        ans = f\"Question: {query}\\nLLM Answer: \" + ans\n    elif CFG.task_type == TaskType.CAUSAL_LM:\n        ans = ans.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").strip()\n        ans = re.sub(r\" +\", \" \", ans)\n    \n    return ans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:18:22.854678Z","iopub.execute_input":"2024-12-31T09:18:22.855034Z","iopub.status.idle":"2024-12-31T09:18:22.859601Z","shell.execute_reply.started":"2024-12-31T09:18:22.855004Z","shell.execute_reply":"2024-12-31T09:18:22.858710Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"# Evaluations\n- Check model on a single sample\n- Calculate average recall score on validation dataset\n- Check wrong answers","metadata":{}},{"cell_type":"code","source":"# Load the ROUGE metric\nmetric = evaluate.load(\"rouge\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:18:26.266785Z","iopub.execute_input":"2024-12-31T09:18:26.267093Z","iopub.status.idle":"2024-12-31T09:18:27.239807Z","shell.execute_reply.started":"2024-12-31T09:18:26.267069Z","shell.execute_reply":"2024-12-31T09:18:27.239113Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def extract_prediction(llm_output):\n    return llm_output.split(\"Answer:\")[1].split(\"Sources:\")[0].strip()\n\ndef evaluate_answer(dataset, indx):\n    # Get the question and correct answer from the DataFrame\n    query = dataset[indx]['Question']\n    correct_answer = dataset[indx]['Answer']\n\n    # Get the predicted answer from the language model\n    pred_ans = llm_ans(query)\n\n    print(pred_ans)\n    print(f\"\\nCorrect Answer: {correct_answer}\")\n\n    # Compute ROUGE scores\n    rouge_score = metric.compute(\n        predictions=[extract_prediction(pred_ans)],\n        references=[correct_answer],\n        use_stemmer=True\n    )\n\n    print(f\"\\nROUGE Recall Scores: {rouge_score}\")\n\nevaluate_answer(eval_dataset, 6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:18:28.435078Z","iopub.execute_input":"2024-12-31T09:18:28.435362Z","iopub.status.idle":"2024-12-31T09:18:29.892890Z","shell.execute_reply.started":"2024-12-31T09:18:28.435338Z","shell.execute_reply":"2024-12-31T09:18:29.892003Z"}},"outputs":[{"name":"stdout","text":"Use only the following pieces of context to answer the question.\n\nrepublic, which could have risked the loss of the Southwest or dominance of the Northeast. Monroe placed faith in a strong presidency and the system of checks and balances. In the 1790s he fretted over an aging George Washington being too heavily influenced by close advisers like Hamilton who was too close to Britain. Monroe opposed the Jay Treaty and was humiliated when Washington criticized for\n\nnegotiate the Louisiana Purchase. Monroe was then appointed Minister to the Court of St. James (Britain) from 1803 to 1807. In 1806 he negotiated a treaty with Britain to replace the Jay Treaty of 1794, but Jefferson rejected it as unsatisfactory, as the treaty contained no ban on the British practice of impressment of American sailors. As a result, the two nations moved closer toward the War of\n\nQuestion: James Monroe. What was the result of the rejection of the Jay Treaty?\nAnswer: The two nations moved closer toward war\n\nSources: \nS08_set3_a2.txt.c\nContent: republic, which could have risked the loss of the Southwest or dominance of the Northeast. Monroe placed faith in a strong presidency and the system of checks and balances. In the 1790s he fretted over an aging George Washington being too heavily influenced by close advisers like Hamilton who was too close to Britain. Monroe opposed the Jay Treaty and was humiliated when Washington criticized for \nS08_set3_a2.txt.c\nContent: negotiate the Louisiana Purchase. Monroe was then appointed Minister to the Court of St. James (Britain) from 1803 to 1807. In 1806 he negotiated a treaty with Britain to replace the Jay Treaty of 1794, but Jefferson rejected it as unsatisfactory, as the treaty contained no ban on the British practice of impressment of American sailors. As a result, the two nations moved closer toward the War of\n\nCorrect Answer: As a result, the two nations moved closer toward the War of 1812. \n\nROUGE Recall Scores: {'rouge1': 0.7000000000000001, 'rouge2': 0.5555555555555556, 'rougeL': 0.7000000000000001, 'rougeLsum': 0.7000000000000001}\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# Define a function to make predictions\ndef predict(batch, llm_ans, metric):\n    queries = batch['Question']\n    \n    # Get the predicted answers from the model for the entire batch\n    pred_ans = [llm_ans(query) for query in queries]\n    \n    # Extract predictions from LLM output\n    extracted_preds = [extract_prediction(ans) for ans in pred_ans]\n    \n    # Initialize lists to store ROUGE scores\n    recalls = []\n    \n    # Calculate ROUGE score for each prediction and store recalls\n    for pred, ref in zip(extracted_preds, batch['Answer']):\n        result = metric.compute(predictions=[pred], references=[ref])\n        recalls.append(result['rouge1'])\n    \n    # Return predictions, references, and low recall indices\n    return {\n        'prediction': extracted_preds, \n        'reference': batch['Answer'],\n        'recalls': recalls\n    }\n\n# Apply the function to all rows in the dataset\npredicted_dataset = eval_dataset.map(\n    lambda batch: predict(batch, llm_ans=llm_ans, metric=metric),\n    batched=True,\n    batch_size=16,\n    desc=\"Processing predictions\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:19:00.244194Z","iopub.execute_input":"2024-12-31T09:19:00.244497Z","iopub.status.idle":"2024-12-31T09:21:03.832196Z","shell.execute_reply.started":"2024-12-31T09:19:00.244472Z","shell.execute_reply":"2024-12-31T09:21:03.831496Z"}},"outputs":[{"name":"stderr","text":"Parameter 'function'=<function <lambda> at 0x79cf07e38790> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing predictions:   0%|          | 0/121 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d408612485974f68908e31b3c5836c3e"}},"metadata":{}},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# Compute the ROUGE score for the entire dataset\nrouge_score = metric.compute(\n    predictions=predicted_dataset['prediction'],\n    references=predicted_dataset['reference'],\n    use_aggregator=True,\n    use_stemmer=True\n)\n\nprint(f\"ROUGE Recall Scores: {rouge_score}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:21:09.372694Z","iopub.execute_input":"2024-12-31T09:21:09.373064Z","iopub.status.idle":"2024-12-31T09:21:09.566231Z","shell.execute_reply.started":"2024-12-31T09:21:09.373033Z","shell.execute_reply":"2024-12-31T09:21:09.565383Z"}},"outputs":[{"name":"stdout","text":"ROUGE Recall Scores: {'rouge1': 0.6879041979198757, 'rouge2': 0.18036733749235295, 'rougeL': 0.6873187776070315, 'rougeLsum': 0.6877324688225266}\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# Collect the low recall indices based on recalls < 0.5\nlow_recall_indices = [i for i, recall in enumerate(predicted_dataset['recalls']) if recall < 0.5]\nwrong_ans = eval_dataset.select(low_recall_indices)\n\n# Display the filtered dataframe\npercentage_wrong = round((len(wrong_ans) / len(eval_dataset)) * 100)\nprint(f\"Number of wrong answers: {len(wrong_ans)} ({percentage_wrong}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:21:17.565088Z","iopub.execute_input":"2024-12-31T09:21:17.565421Z","iopub.status.idle":"2024-12-31T09:21:17.581185Z","shell.execute_reply.started":"2024-12-31T09:21:17.565392Z","shell.execute_reply":"2024-12-31T09:21:17.580339Z"}},"outputs":[{"name":"stdout","text":"Number of wrong answers: 38 (31%)\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"evaluate_answer(wrong_ans, 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T09:23:29.934956Z","iopub.execute_input":"2024-12-31T09:23:29.935241Z","iopub.status.idle":"2024-12-31T09:23:30.736239Z","shell.execute_reply.started":"2024-12-31T09:23:29.935218Z","shell.execute_reply":"2024-12-31T09:23:30.735483Z"}},"outputs":[{"name":"stdout","text":"Use only the following pieces of context to answer the question.\n\nJames_Monroe James Monroe (April 28, 1758 â July 4, 1831) was the fifth President of the United States (1817-1825). His administration was marked by the acquisition of Florida (1819); the Missouri Compromise (1820), in which Missouri was declared a slave state; and the profession of the Monroe Doctrine (1823), declaring U.S. opposition to European interference in the Americas. The Presidentâs\n\nfounded by the American Colonization Society, in 1822, as a haven for freed slaves. * Monroe was (arguably) the last president to have fought in the Revolutionary War, although Andrew Jackson served as a 13-year-old courier in the Continental Army and was taken as a prisoner of war by the British. * Monroe is considered to be the president who was in the most paintings; throughout the 1800s he\n\nQuestion: James Monroe. What is the first number on the page?\nAnswer: April\n\nSources: \nS08_set3_a2.txt.c\nContent: James_Monroe James Monroe (April 28, 1758 â July 4, 1831) was the fifth President of the United States (1817-1825). His administration was marked by the acquisition of Florida (1819); the Missouri Compromise (1820), in which Missouri was declared a slave state; and the profession of the Monroe Doctrine (1823), declaring U.S. opposition to European interference in the Americas. The Presidentâs \nS08_set3_a2.txt.c\nContent: founded by the American Colonization Society, in 1822, as a haven for freed slaves. * Monroe was (arguably) the last president to have fought in the Revolutionary War, although Andrew Jackson served as a 13-year-old courier in the Continental Army and was taken as a prisoner of war by the British. * Monroe is considered to be the president who was in the most paintings; throughout the 1800s he\n\nCorrect Answer: 28\n\nROUGE Recall Scores: {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"# Conclusions\n\n- Things I found had the most impact on models output quality in my experiments:\n    - Splitting: chunk size, overlap\n    - Search: k\n    - Pipeline parameters (temperature, top_p, penalty)\n    - Embeddings function\n    - Question with or without title","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}